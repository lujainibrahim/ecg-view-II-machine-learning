{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import tensorflow as tf\n",
    "from numpy import savetxt\n",
    "from tensorflow.python.framework import ops\n",
    "print(tf.__version__)\n",
    "\n",
    "#Visualization Libraries\n",
    "import seaborn as sns\n",
    "\n",
    "# Size of matplotlib histogram bins\n",
    "bin_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "# load array\n",
    "train = loadtxt('train.csv', delimiter=',')\n",
    "test = loadtxt('test.csv', delimiter=',')\n",
    "\n",
    "# Split array\n",
    "train_x = train[:,:11]\n",
    "test_x = test[:,:11]\n",
    "train_y = train[:,11]\n",
    "test_y = test[:,11]\n",
    "\n",
    "x_re = np.vstack((train_x, test_x))\n",
    "y_re = np.vstack((train_y, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers import Layer, GRU, LSTM, Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import regularizers, backend, initializers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from keras.initializers import Ones, Zeros\n",
    "import keras.backend as K\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Define Layer Normalization class\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "layer_size1 = 12\n",
    "layer_size2 = 10\n",
    "layer_size3 = 7\n",
    "layer_size4 = 5\n",
    "layer_size5 = 4\n",
    "layer_size6 = 3\n",
    "timesteps = 1 # static data\n",
    "data_dim = 11\n",
    "\n",
    "X_train = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\n",
    "X_test = np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))\n",
    "#train_y = to_categorical(train_y)\n",
    "\n",
    "#  use_bias=True, bias_initializer=initializers.Constant(-1)\n",
    "\n",
    "def create_model(learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(layer_size1, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size2, return_sequences=True))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size3, return_sequences=True))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size4, return_sequences=True))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size5, return_sequences=True))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size6, return_sequences=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the optimal parameters \n",
    "model = create_model(learning_rate = 0.001)\n",
    "\n",
    "# Stop the training when there is no improvement in the validation accuracy for ten consecutive epochs.\n",
    "early = EarlyStopping(monitor='val_accuracy', patience=10, verbose=0)\n",
    "\n",
    "# reduces learning rate when a metric has stopped improving\n",
    "redonplat = ReduceLROnPlateau(monitor='val_accuracy', mode=\"max\", patience=7, verbose=0)\n",
    "\n",
    "# defining the callbacks list to include the above parameters\n",
    "callbacks_list = [early, redonplat]\n",
    "\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, train_y, epochs=1000, verbose=0, callbacks=callbacks_list, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.824696564178524 \n",
      "Test ROC AUC score : 0.8248306853888278 \n",
      "Test accuracy score : 0.844229278163354 \n",
      "--- 4.864065408706665 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# test the model\n",
    "pred_test = model.predict(X_test)\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "\n",
    "# get f1 score of the model & print it. The f1 score considers the precision & recall.\n",
    "f1 = f1_score(test_y, pred_test, average=\"macro\")\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "# get ROC AUC score of the model & print it\n",
    "roc = roc_auc_score(test_y, pred_test)\n",
    "print(\"Test ROC AUC score : %s \"% roc)\n",
    "\n",
    "# get the accuracy and print it\n",
    "acc = accuracy_score(test_y, pred_test)\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity : 0.8828498136311456 \n",
      "Sensitivity: 0.7668115571465098 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# get the specificity\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, pred_test).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "print(\"Specificity : %s \"% specificity)\n",
    "\n",
    "# get the sensitivity\n",
    "sensitivity= tp / (tp+fn)\n",
    "print(\"Sensitivity: %s \"% sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 83.83%\n",
      "accuracy: 83.80%\n",
      "accuracy: 84.29%\n",
      "accuracy: 84.04%\n",
      "accuracy: 83.96%\n",
      "accuracy: 83.89%\n",
      "accuracy: 83.22%\n",
      "accuracy: 84.02%\n",
      "accuracy: 83.90%\n",
      "accuracy: 83.69%\n",
      "83.86% (+/- 0.27%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy\n",
    "\n",
    "# 10-fold cross validation on the test data \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Use the whole dataset\n",
    "X_re = np.reshape(x_re, (x_re.shape[0], 1, x_re.shape[1]))\n",
    "#y_re = to_categorical(y_re)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X_re, y_re):\n",
    "    model = create_model(learning_rate = 0.001)\n",
    "    # Fit the model\n",
    "    model.fit(X_re[train], to_categorical(y_re[train]), epochs=1000, verbose=0, callbacks=callbacks_list, validation_split=0.1)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_re[test], to_categorical(y_re[test]), verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
