{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers import Layer, GRU, LSTM, Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import regularizers, backend, initializers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Ones, Zeros\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from numpy import savetxt\n",
    "from tensorflow.python.framework import ops\n",
    "print(tf.__version__)\n",
    "\n",
    "# Visualization Libraries\n",
    "import seaborn as sns\n",
    "\n",
    "# Size of matplotlib histogram bins\n",
    "bin_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load array\n",
    "train = loadtxt('train.csv', delimiter=',')\n",
    "test = loadtxt('test.csv', delimiter=',')\n",
    "\n",
    "# Split array\n",
    "train_x = train[:,:11]\n",
    "test_x = test[:,:11]\n",
    "train_y = train[:,11]\n",
    "test_y = test[:,11]\n",
    "\n",
    "x_re = np.vstack((train_x, test_x))\n",
    "y_re = np.vstack((train_y, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Layer Normalization class\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "layer_size1 = 12\n",
    "layer_size2 = 10\n",
    "layer_size3 = 7\n",
    "layer_size4 = 5\n",
    "layer_size5 = 4\n",
    "layer_size6 = 3\n",
    "timesteps = 1 # static data\n",
    "data_dim = 11\n",
    "\n",
    "X_train = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\n",
    "X_test = np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))\n",
    "\n",
    "\n",
    "def create_model(learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(layer_size1, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size2, return_sequences=True))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size3, return_sequences=True))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size4, return_sequences=True))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size5, return_sequences=True))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(GRU(layer_size6, return_sequences=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the optimal parameters \n",
    "model = create_model(learning_rate = 0.001)\n",
    "\n",
    "# Set early stopping based on accuracy. It stops after 10 consecutive epochs of no accuracy improvement.\n",
    "early = EarlyStopping(monitor='val_accuracy', patience=10, verbose=0)\n",
    "\n",
    "# Reduce learning rate based on accuracy. It reduces the rate after 7 consecutive epochs of no accuracy improvement.\n",
    "redonplat = ReduceLROnPlateau(monitor='val_accuracy', mode=\"max\", patience=7, verbose=0)\n",
    "\n",
    "callbacks_list = [early, redonplat]\n",
    "\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, train_y, epochs=1000, verbose=0, callbacks=callbacks_list, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.824696564178524 \n",
      "Test ROC AUC score : 0.8248306853888278 \n",
      "Test accuracy score : 0.844229278163354 \n",
      "--- 4.864065408706665 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Test the model\n",
    "pred_test = model.predict(X_test)\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "\n",
    "# Get f1 score \n",
    "f1 = f1_score(test_y, pred_test, average=\"macro\")\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "# Get ROC AUC score \n",
    "roc = roc_auc_score(test_y, pred_test)\n",
    "print(\"Test ROC AUC score : %s \"% roc)\n",
    "\n",
    "# Get the accuracy\n",
    "acc = accuracy_score(test_y, pred_test)\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity : 0.8828498136311456 \n",
      "Sensitivity: 0.7668115571465098 \n"
     ]
    }
   ],
   "source": [
    "# Get the specificity\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, pred_test).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "print(\"Specificity : %s \"% specificity)\n",
    "\n",
    "# Get the sensitivity\n",
    "sensitivity= tp / (tp+fn)\n",
    "print(\"Sensitivity: %s \"% sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 83.83%\n",
      "accuracy: 83.80%\n",
      "accuracy: 84.29%\n",
      "accuracy: 84.04%\n",
      "accuracy: 83.96%\n",
      "accuracy: 83.89%\n",
      "accuracy: 83.22%\n",
      "accuracy: 84.02%\n",
      "accuracy: 83.90%\n",
      "accuracy: 83.69%\n",
      "83.86% (+/- 0.27%)\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross validation on the test data \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Use the whole dataset\n",
    "X_re = np.reshape(x_re, (x_re.shape[0], 1, x_re.shape[1]))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X_re, y_re):\n",
    "    model = create_model(learning_rate = 0.001)\n",
    "    # Fit the model\n",
    "    model.fit(X_re[train], to_categorical(y_re[train]), epochs=1000, verbose=0, callbacks=callbacks_list, validation_split=0.1)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_re[test], to_categorical(y_re[test]), verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
